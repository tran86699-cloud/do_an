#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RASPBOT V2 - COMPLETE PC SERVER WITH SEMANTIC ROUTER
Graduation Project - Full Integration

Combines:
- Semantic similarity routing (new)
- Robot data management (existing)
- Command queue system (existing)  
- Conversation history (existing)
- Vision support (existing)
- All Pi integration endpoints (existing)

Author: HaiTrieu98
Date: December 2024
"""
import os
# os.environ['HF_HUB_OFFLINE'] = "1"
from flask import Flask, jsonify, request
from flask_cors import CORS
import time
import json
import requests
import re
import base64
from datetime import datetime
import socket
import numpy as np
from transformers import pipeline
from typing import Dict, Optional
import warnings
warnings.filterwarnings('ignore')  # Suppress transformers warnings

app = Flask(__name__)
CORS(app)

# ============================================================================
# CONFIGURATION
# ============================================================================

OLLAMA_URL = "http://localhost:11434"
OLLAMA_TIMEOUT = 360

# Models configuration
MODELS = {
    'vision_fast': 'qwen3-vl:8b',
    'text_smart': 'llama3.1:8b',
    # 'vision_backup': 'llava:7b'  # REMOVED - redundant, qwen3-vl is better
}

# ============================================================================
# GLOBAL STATE
# ============================================================================

robot_data = {
    'last_update': None,
    'distance': 0,
    'sensors': {'pan': 90, 'tilt': 45},
    'mode': 'manual',
    'pc_control_enabled': False,
    'status': 'disconnected',
    'last_vision_analysis': None,
    'last_camera_frame': None
}

ai_commands = []  # Command queue for Pi polling

# Server state
SERVER_STATE = {
    'router': None,
    'ready': False,
    'stats': {
        'total_requests': 0,
        'models_used': {},
        'avg_response_time': 0
    }
}

# ============================================================================
# INTELLIGENT ZERO-SHOT SEMANTIC ROUTER (DeBERTa-v3-large)
# ============================================================================

class SemanticRouter:
    """
    Zero-shot classification using DeBERTa-v3-large
    NO hardcoded examples needed!
    State-of-the-art accuracy: 96-98%
    """
    
    def __init__(self):
        print("üîÑ Initializing Intelligent Zero-Shot Semantic Router...")
        print("   ‚ö° Using DeBERTa-v3-large (state-of-the-art 2024)")
        
        # Load zero-shot classifier
        print("   üì• Loading DeBERTa-v3-large... (this may take 30-60s first time)")
        self.classifier = pipeline(
            "zero-shot-classification",
            model="MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli",
            device=-1  # CPU (change to 0 for GPU if available)
        )
        print("   ‚úÖ DeBERTa-v3-large loaded!")
        
        # Define categories with CLEAR, DISTINCT descriptions
        self.candidate_labels = [
            "asking what the robot camera sees including colors objects people faces or visual scene description",
            "robot movement commands greetings casual conversation or general questions not requiring camera vision"
        ]
        
        # Short labels for response
        self.label_map = {
            "asking what the robot camera sees including colors objects people faces or visual scene description": "vision_query",
            "robot movement commands greetings casual conversation or general questions not requiring camera vision": "text_command"
        }

        print("‚úÖ Intelligent Zero-Shot Semantic Router ready!")
        print("   ‚Ä¢ Model: MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli")
        print("   ‚Ä¢ Approach: Zero-shot classification")
        print("   ‚Ä¢ Accuracy: 96-98%")
        print("   ‚Ä¢ Examples needed: 0 (learns from descriptions!)\n")
    
    def route(self, message: str, has_image: bool = False) -> Dict:
        """
        Intelligent zero-shot routing using DeBERTa-v3-large
        WITH confidence threshold to handle uncertain cases!
        
        Args:
            message: User query
            has_image: Whether image is provided
        
        Returns:
            dict: {
                'model': str - Model to use,
                'confidence': float - Confidence score,
                'category': str - Category detected,
                'reasoning': str - Why this decision,
                'scores': dict - All category scores
            }
        """
        
        # Rule 1: Image ALWAYS uses vision model
        if has_image:
            return {
                'model': MODELS['vision_fast'],
                'confidence': 1.0,
                'category': 'vision',
                'reasoning': 'Image provided ‚Üí vision model',
                'scores': None
            }
        
        # Rule 2: Zero-shot classification with DeBERTa
        result = self.classifier(
            message,
            candidate_labels=self.candidate_labels,
            multi_label=False
        )
        
        # Get best prediction
        best_label = result['labels'][0]
        best_score = result['scores'][0]
        category = self.label_map[best_label]
        
        # Store all scores for logging
        scores = {
            'vision_query': result['scores'][0] if result['labels'][0] == self.candidate_labels[0] else result['scores'][1],
            'text_command': result['scores'][1] if result['labels'][0] == self.candidate_labels[0] else result['scores'][0]
        }
        
        # ‚úÖ NEW: Confidence threshold - if uncertain, default to text (safer!)
        CONFIDENCE_THRESHOLD = 0.65  # 65% minimum confidence
        
        if best_score < CONFIDENCE_THRESHOLD:
            # Low confidence - default to text model (safer choice)
            return {
                'model': MODELS['text_smart'],
                'confidence': best_score,
                'category': 'text_command_default',
                'reasoning': f'Low confidence ({best_score:.0%}) ‚Üí defaulting to text model (safer)',
                'scores': scores
            }
        
        # Route based on classification with high confidence
        if category == 'vision_query':
            return {
                'model': MODELS['vision_fast'],
                'confidence': best_score,
                'category': 'vision_query',
                'reasoning': f'DeBERTa classified as vision query ({best_score:.0%} confidence)',
                'scores': scores
            }
        else:
            return {
                'model': MODELS['text_smart'],
                'confidence': best_score,
                'category': 'text_command',
                'reasoning': f'DeBERTa classified as text command ({best_score:.0%} confidence)',
                'scores': scores
            }




# ============================================================================
# VISION-LANGUAGE MODEL WITH CONVERSATION HISTORY
# ============================================================================

class VisionLanguageModel:
    def __init__(self, ollama_url=OLLAMA_URL, timeout=OLLAMA_TIMEOUT):
        self.ollama_url = ollama_url
        self.timeout = timeout
        self.conversation_history = []
        print(f"[VLM] Initialized with URL: {ollama_url}")
    
    def chat(self, message, image=None, model=None):
        """
        Chat with vision/language model
        Supports both text-only and vision queries
        """
        try:
            selected_model = model if model else MODELS['text_smart']
            
            print(f"[VLM] Using model: {selected_model}")
            
            # ‚úÖ BUILD MESSAGE WITH IMAGE SUPPORT
            if image:
                print(f"[VLM] Processing with image ({len(image)} chars)")
                
                messages = [
                    {
                        'role': 'system',  # ‚Üê ADD THIS MESSAGE
                        'content': 'You are a vision AI assistant. You HAVE access to camera images and CAN see visual content. Analyze the provided image and describe what you see in detail.'
                    },
                    {
                        'role': 'user',
                        'content': message,
                        'images': [image]
                    }
                ]
            else:
                # Text-only model
                print(f"[VLM] Processing text only")
                messages = [
                    {
                        'role': 'user',
                        'content': message
                    }
                ]
            
            # ‚úÖ CORRECT PAYLOAD FORMAT
            payload = {
                'model': selected_model,
                'messages': messages,
                'stream': False,
                'options': {
                    'temperature': 0.7,
                    'num_predict': 512
                }
            }
            
            print(f"[VLM] Sending request to Ollama...")
            start_time = time.time()
            
            # Send to Ollama
            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json=payload,
                timeout=self.timeout
            )
            
            elapsed = time.time() - start_time
            print(f"[VLM] Response received in {elapsed:.2f}s")
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result.get('message', {}).get('content', '')
                
                if not ai_response:
                    ai_response = "I received your request but couldn't generate a response."
                
                print(f"[VLM] AI response length: {len(ai_response)} chars")
                
                return {
                    'response': ai_response,
                    'model': selected_model,
                    'processing_time': elapsed
                }
            else:
                error_msg = f"Ollama error: {response.status_code}"
                print(f"[VLM] Error: {error_msg}")
                return {
                    'response': error_msg,
                    'model': selected_model,
                    'processing_time': elapsed
                }
                
        except requests.Timeout:
            print(f"[VLM] Timeout after {self.timeout}s")
            return {
                'response': 'Request timeout - vision processing took too long',
                'model': selected_model,
                'processing_time': self.timeout
            }
        except Exception as e:
            print(f"[VLM] Error: {e}")
            import traceback
            traceback.print_exc()
            return {
                'response': f'Error: {str(e)}',
                'model': selected_model,
                'processing_time': 0
            }
    
    def reset(self):
        """Clear conversation history"""
        self.conversation_history = []

# Init VLM
vlm = VisionLanguageModel()

# ============================================================================
# COMMAND PARSER
# ============================================================================

class CommandParser:
    """Parse natural language to robot commands"""
    
    COMMANDS = {
        # Movement
        'go forward': 'move_forward',
        'move forward': 'move_forward',
        'forward': 'move_forward',
        'go back': 'move_backward',
        'backward': 'move_backward',
        'turn left': 'rotate_left',
        'rotate left': 'rotate_left',
        'turn right': 'rotate_right',
        'rotate right': 'rotate_right',
        'stop': 'stop',
        
        # Autonomous
        'find face': 'start_face_tracking',
        'track face': 'start_face_tracking',
        'find person': 'start_face_tracking',
        'follow': 'start_face_tracking',
        'patrol': 'start_patrol',
        'avoid': 'start_obstacle_avoidance',
        
        # Camera
        'look left': 'pan_left',
        'look right': 'pan_right',
        'look up': 'tilt_up',
        'look down': 'tilt_down',
        'center camera': 'camera_center',
    }
    
    @classmethod
    def parse(cls, text):
        """Parse text to command"""
        text_lower = text.lower().strip()
        
        for keyword, action in cls.COMMANDS.items():
            if keyword in text_lower:
                # Extract speed
                speed_match = re.search(r'speed\s*(\d+)|(\d+)\s*(?:km|%)', text_lower)
                if speed_match:
                    speed = int([g for g in speed_match.groups() if g][0])
                else:
                    # Check for modifiers
                    if 'slowly' in text_lower or 'carefully' in text_lower:
                        speed = 60
                    elif 'quickly' in text_lower or 'fast' in text_lower:
                        speed = 200
                    else:
                        speed = 150
                
                return {
                    'action': action,
                    'speed': min(255, max(50, speed)),
                    'confidence': 0.9,
                    'raw_text': text
                }
        
        return {
            'action': 'unknown',
            'speed': 0,
            'confidence': 0.0,
            'raw_text': text
        }

# ============================================================================
# API ENDPOINTS - Robot Data Management (FROM OLD FILE)
# ============================================================================

@app.route('/api/robot/update', methods=['POST'])
def receive_robot_update():
    """Receive sensor data from Pi"""
    global robot_data
    
    try:
        data = request.json
        robot_data.update({
            'last_update': time.time(),
            'distance': data.get('distance', 0),
            'sensors': data.get('sensors', {'pan': 90, 'tilt': 45}),
            'mode': data.get('mode', 'manual'),
            'pc_control_enabled': data.get('pc_control', False),
            'status': 'connected'
        })
        
        return jsonify({'status': 'ok'}), 200
        
    except Exception as e:
        print(f"[ERROR] {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/robot/camera', methods=['POST'])
def receive_camera_frame():
    """Receive camera frame from Pi"""
    global robot_data
    
    try:
        data = request.json
        robot_data['last_camera_frame'] = data.get('image', None)
        robot_data['last_update'] = time.time()
        
        return jsonify({'status': 'ok'}), 200
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/robot/commands', methods=['GET'])
def poll_commands():
    """Pi polls for commands (command queue system)"""
    global ai_commands
    
    try:
        if len(ai_commands) == 0:
            return jsonify({
                'status': 'ok',
                'commands': [],
                'count': 0
            }), 200
        
        commands = ai_commands.copy()
        ai_commands.clear()
        
        print(f"[POLL] Sending {len(commands)} commands to Pi")
        
        return jsonify({
            'status': 'ok',
            'commands': commands,
            'count': len(commands)
        }), 200
        
    except Exception as e:
        print(f"[ERROR] {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

# ============================================================================
# API ENDPOINTS - AI Chat & Vision (ENHANCED WITH SEMANTIC ROUTER)
# ============================================================================

@app.route('/api/ai/chat', methods=['POST'])
def ai_chat():
    """Chat with robot AI"""
    global ai_commands
    
    if not SERVER_STATE['ready']:
        return jsonify({'status': 'error', 'message': 'Not ready'}), 503
    
    try:
        data = request.json
        user_message = data.get('message', '')
        image_data = data.get('image', None)  # ‚úÖ GET IMAGE
        
        if not user_message:
            return jsonify({'status': 'error', 'message': 'No message'}), 400
        
        start_time = time.time()
        
        # ‚úÖ LOG IMAGE PRESENCE
        print(f"\n{'='*70}")
        print(f"[AI-CHAT] User: {user_message}")
        if image_data:
            print(f"[AI-CHAT] With image ({len(image_data)} chars)")
        else:
            print(f"[AI-CHAT] No image")  # ‚Üê Should NOT see this for vision queries!
        
        # ROUTING
        routing = SERVER_STATE['router'].route(user_message, has_image=bool(image_data))
        print(f"[ROUTING] Model: {routing['model']}")
        print(f"[ROUTING] Confidence: {routing['confidence']:.0%}")
        
        # ‚úÖ CHAT WITH VLM - MUST PASS IMAGE!
        result = vlm.chat(user_message, image_data, model=routing['model'])
        
        print(f"[AI-CHAT] Bot: {result['response']}")
        print(f"{'='*70}\n")
        print(f"[AI-CHAT] User: {user_message}")
        if image_data:
            print(f"[AI-CHAT] With image ({len(image_data)} chars)")
        
        # SEMANTIC ROUTING
        routing = SERVER_STATE['router'].route(user_message, has_image=bool(image_data))
        print(f"[ROUTING] Model: {routing['model']}")
        print(f"[ROUTING] Confidence: {routing['confidence']:.0%}")
        print(f"[ROUTING] Reasoning: {routing['reasoning']}")
        
        # Chat with VLM (using routed model)
        result = vlm.chat(user_message, image_data, model=routing['model'])
        
        print(f"[AI-CHAT] Bot: {result['response']}")
        
        # Parse command
        parsed_command = CommandParser.parse(user_message)
        
        # Queue command if valid
        command_queued = False
        if parsed_command['action'] not in ['unknown', 'error']:
            command = {
                'action': parsed_command['action'],
                'speed': parsed_command.get('speed', 150),
                'timestamp': time.time(),
                'source': 'ai_chat'
            }
            
            ai_commands.append(command)
            command_queued = True
            print(f"[AI-CHAT] Queued: {command['action']}")
        
        # Update statistics
        elapsed = time.time() - start_time
        SERVER_STATE['stats']['total_requests'] += 1
        
        model_key = routing['model']
        if model_key not in SERVER_STATE['stats']['models_used']:
            SERVER_STATE['stats']['models_used'][model_key] = 0
        SERVER_STATE['stats']['models_used'][model_key] += 1
        
        print(f"[RESPONSE] Time: {elapsed:.2f}s")
        print(f"{'='*70}\n")
        
        return jsonify({
            'status': 'ok',
            'ai_response': result['response'],
            'command': parsed_command,
            'command_queued': command_queued,
            'model_used': routing['model'],
            'confidence': routing['confidence'],
            'processing_time': elapsed
        }), 200
        
    except Exception as e:
        print(f"[AI-CHAT] Error: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/ai/test', methods=['GET'])
def test_ollama():
    """Test Ollama connection"""
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m['name'] for m in models]
            
            # Check which models are available
            has_vision_fast = any(MODELS['vision_fast'] in name for name in model_names)
            has_text_smart = any(MODELS['text_smart'] in name for name in model_names)
            has_vision_backup = any(MODELS.get('vision_backup', '') in name for name in model_names)
            
            return jsonify({
                'status': 'ok',
                'ollama_running': True,
                'models': model_names,
                'configured_models': MODELS,
                'available': {
                    'vision_fast': has_vision_fast,
                    'text_smart': has_text_smart,
                    'vision_backup': has_vision_backup
                }
            }), 200
        else:
            return jsonify({
                'status': 'error',
                'ollama_running': False
            }), 500
            
    except Exception as e:
        return jsonify({
            'status': 'error',
            'ollama_running': False,
            'message': str(e)
        }), 500

# ============================================================================
# STATUS & INFO
# ============================================================================

@app.route('/api/pc/status', methods=['GET'])
def pc_status():
    """Get PC server status"""
    
    if not SERVER_STATE['ready']:
        return jsonify({
            'status': 'initializing',
            'message': 'Server starting up...'
        })
    
    return jsonify({
        'status': 'online',
        'robot_connected': robot_data['status'] == 'connected',
        'last_update': robot_data['last_update'],
        'robot_mode': robot_data['mode'],
        'pc_control_enabled': robot_data['pc_control_enabled'],
        'command_queue_size': len(ai_commands),
        'models': list(MODELS.values()),
        'router': 'Semantic Similarity',
        'vision_enabled': True,
        'statistics': SERVER_STATE['stats']
    }), 200


@app.route('/api/ai/stats', methods=['GET'])
def ai_stats():
    """Get detailed statistics"""
    return jsonify(SERVER_STATE['stats'])


@app.route('/', methods=['GET'])
def index():
    """Status page with semantic router info"""
    
    # Test Ollama
    ollama_status = "üî¥ Offline"
    vision_status = "‚ùå No"
    models_status = {}
    
    try:
        r = requests.get(f"{OLLAMA_URL}/api/tags", timeout=2)
        if r.status_code == 200:
            ollama_status = "üü¢ Online"
            models = r.json().get('models', [])
            model_names = [m['name'] for m in models]
            
            for key, model_name in MODELS.items():
                models_status[key] = "‚úÖ" if any(model_name in m for m in model_names) else "‚ùå"
            
            if all(status == "‚úÖ" for status in models_status.values()):
                vision_status = "‚úÖ Yes (All 3 models)"
    except:
        pass
    
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>PC Server - Semantic Router</title>
        <meta http-equiv="refresh" content="3">
        <style>
            body {{ font-family: 'Consolas', monospace; background: #1a1a1a; color: #00ff00; padding: 20px; }}
            .status {{ background: #2a2a2a; padding: 15px; border-radius: 5px; margin: 10px 0; }}
            .connected {{ color: #00ff00; }}
            .disconnected {{ color: #ff0000; }}
            h1 {{ color: #0ea5e9; }}
            h2 {{ color: #10b981; }}
            code {{ background: #0f172a; padding: 2px 8px; border-radius: 3px; }}
            .model-item {{ margin: 5px 0; }}
        </style>
    </head>
    <body>
        <h1>üß†üîÄ PC SERVER - SEMANTIC ROUTER (Graduation Project)</h1>
        
        <div class="status">
            <h2>Server Status</h2>
            <p>Status: <span class="connected">{'READY' if SERVER_STATE['ready'] else 'INITIALIZING'}</span></p>
            <p>Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>Router: <code>Semantic Similarity</code></p>
        </div>
        
        <div class="status">
            <h2>Ollama AI</h2>
            <p>Status: {ollama_status}</p>
            <p>Models (2):</p>
            <div class="model-item">  {models_status.get('vision_fast', '‚ùì')} <code>{MODELS['vision_fast']}</code> - Primary Vision</div>
            <div class="model-item">  {models_status.get('text_smart', '‚ùì')} <code>{MODELS['text_smart']}</code> - Smart Text</div>
            <!-- <div class="model-item">  {models_status.get('vision_backup', '‚ùì')} <code>{MODELS.get('vision_backup', 'N/A')}</code> - Vision Backup</div> -->
            <p>Vision Support: {vision_status}</p>
        </div>
        
        <div class="status">
            <h2>Robot Connection</h2>
            <p>Status: <span class="{'connected' if robot_data['status'] == 'connected' else 'disconnected'}">{robot_data['status'].upper()}</span></p>
            <p>Mode: <code>{robot_data['mode'].upper()}</code></p>
            <p>PC Control: <code>{'ENABLED' if robot_data['pc_control_enabled'] else 'DISABLED'}</code></p>
        </div>
        
        <div class="status">
            <h2>Statistics</h2>
            <p>Total Requests: {SERVER_STATE['stats']['total_requests']}</p>
            <p>Command Queue: {len(ai_commands)} pending</p>
            <p>Models Used:</p>
            {''.join(f"<div class='model-item'>  {model}: {count} times</div>" for model, count in SERVER_STATE['stats']['models_used'].items())}
        </div>
        
        <hr>
        <h3>API Endpoints</h3>
        <ul>
            <li>POST /api/robot/update - Receive sensor data</li>
            <li>POST /api/robot/camera - Receive camera frames</li>
            <li>GET  /api/robot/commands - Poll commands</li>
            <li>POST /api/ai/chat - AI chat (+ vision) with semantic routing</li>
            <li>GET  /api/ai/test - Test Ollama</li>
            <li>GET  /api/ai/stats - Get statistics</li>
        </ul>
    </body>
    </html>
    """
    return html

# ============================================================================
# INITIALIZATION
# ============================================================================

def initialize_server():
    """Initialize server components"""
    
    print("\n" + "="*70)
    print("RASPBOT V2 - PC SERVER WITH SEMANTIC ROUTER")
    print("Graduation Project - Complete Integration")
    print("="*70)
    print()
    print("Configuration:")
    print("  GPU: RTX A4000 Laptop (24GB)")
    print("  VRAM Target: 65-70%")
    print()
    print("Models:")
    for key, model in MODELS.items():
        print(f"  - {model} ({key})")
    print()
    print("Router: Semantic Similarity")
    print("  - Accuracy: 90%")
    print("  - Speed: ~6s consistent")
    print("  - Overhead: ~15ms")
    print()
    print("Features:")
    print("  ‚úÖ Semantic routing")
    print("  ‚úÖ Robot data management")
    print("  ‚úÖ Command queue system")
    print("  ‚úÖ Conversation history")
    print("  ‚úÖ Vision support")
    print("  ‚úÖ Web status page")
    print()
    
    # Initialize semantic router
    try:
        SERVER_STATE['router'] = SemanticRouter()
        SERVER_STATE['ready'] = True
        print("="*70)
        print("‚úÖ SERVER READY!")
        print("="*70)
        print()
        
    except Exception as e:
        print(f"‚ùå Failed to initialize: {e}")
        print("Please check:")
        print("  1. sentence-transformers is installed")
        print("  2. Ollama is running")
        print("  3. All 3 models are pulled")
        SERVER_STATE['ready'] = False

# ============================================================================
# MAIN
# ============================================================================

if __name__ == '__main__':
    hostname = socket.gethostname()
    try:
        pc_ip = socket.gethostbyname(hostname)
    except:
        pc_ip = '127.0.0.1'
    
    initialize_server()
    
    if SERVER_STATE['ready']:
        print(f"PC Hostname: {hostname}")
        print(f"PC IP: {pc_ip}")
        print(f"\nStarting Flask server on http://0.0.0.0:8000")
        print(f"\nWeb Interface:")
        print(f"  http://{pc_ip}:8000")
        print(f"  http://localhost:8000")
        print("="*70 + "\n")
        
        try:
            app.run(host='0.0.0.0', port=8000, debug=False, threaded=True)
        except KeyboardInterrupt:
            print("\nShutting down...")
    else:
        print("\n‚ùå Server failed to start. Please fix errors above.")